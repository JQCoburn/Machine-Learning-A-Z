{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z: Section 29 Natural Language Processing - Bag of Words Model\n",
    "\n",
    "Natural Language Processing is the area of Machine Learning that deals with analyzing written and spoken word to understand the meaning. It is a huge area of ongoing research with many of the previously covered algorithms and many more being used to teach computers what a chunk of language means.\n",
    "\n",
    "A common model in NLP is the Bag of Words model which simply looks at all the words in a passage and measures the presence or absence of known key words.\n",
    "\n",
    "In this case we are going to be looking at restaurant reviews and trying to determine whether the review has a positive or negative sentiment.\n",
    "\n",
    "## Step 1 Import and Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Libraries for fast linear algebra and array manipulation\n",
    "import pandas as pd # Import and manage datasets\n",
    "from plotly import __version__ as py__version__\n",
    "import plotly.express as px # Libraries for ploting data\n",
    "import plotly.graph_objects as go # Libraries for ploting data\n",
    "import re # Library for using regular expressions. Will help us clean the text\n",
    "import nltk # Library for doing common NLP operations\n",
    "from nltk.corpus import stopwords # Library for removing irrelevant words\n",
    "from nltk.stem.porter import PorterStemmer # Library condensing words down to the root word\n",
    "from sklearn import __version__ as skl__version__\n",
    "from sklearn.model_selection import train_test_split # Library to split data into training and test sets.\n",
    "from sklearn.naive_bayes import GaussianNB # Library to do Naive Bayes classification\n",
    "from sklearn.ensemble import RandomForestClassifier # Library to do Random Forest classification\n",
    "from sklearn.metrics import confusion_matrix #Function for computing the confusion matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer # Library to create the 'one-hot' encoded word matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library versions used in this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.16.4\n",
      "Pandas: 0.25.1\n",
      "Plotly: 4.0.0\n",
      "NLTK: 3.4.4\n",
      "Scikit-learn: 0.21.2\n"
     ]
    }
   ],
   "source": [
    "print('Numpy: ' + np.__version__)\n",
    "print('Pandas: ' + pd.__version__)\n",
    "print('Plotly: ' + py__version__)\n",
    "print('NLTK: ' + nltk.__version__)\n",
    "print('Scikit-learn: ' + skl__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      Review  Liked\n",
      "0                   Wow... Loved this place.      1\n",
      "1                         Crust is not good.      0\n",
      "2  Not tasty and the texture was just nasty.      0\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 2 columns):\n",
      "Review    1000 non-null object\n",
      "Liked     1000 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 15.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def LoadData():\n",
    "    dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "    return dataset\n",
    "\n",
    "dataset = LoadData()\n",
    "print(dataset.head(3))\n",
    "print()\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now imported the text. In NLP, the choice of delimiter can be important. In this case tab was use since a tab character is unlikely to appear in our text, however this can be very application dependent. And other good delimiter to use can be the pipe '|' character, however in choosing you delimiter you always need to consider what may or may not be in the text you want to delimit.\n",
    "\n",
    "Since language data is almost always very messy (How many typo's do you make on a daily basis?) we will need to clean and prepare the data before we can run it through out model. Also in most languages you have many variations on the same word (love, loves, loved, etc.)\n",
    "\n",
    "The steps in cleaning process are:\n",
    "1. Remove everything that is not a letter (replace with spaces)\n",
    "1. Make everything lowercase\n",
    "1. Split wach review into a list of words\n",
    "1. Remove non-interesting words (ex. a, the, an, and, this, etc.)\n",
    "1. Stem (i.e. keep only the root of similar words)\n",
    "1. Join the cleaned words back together separated by a space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for review in dataset['Review']:\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag of words model works by essentially one-hot encoding each review. This means a new data structure is create with a column for each word that appears in any review. Each row represents a review and the matrix is filled with the count of each word in the review. What this means though is that we will have a very sparse matrix, which can be difficult for algorithms to deal with. Cleaning the input data already helped reduce the number of words (columns) in this matrix and hence the sparsity. In order to reduce it further we will consider only commonly occuring words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = dataset.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our model. We are going to use a Naive Bayes classifier (as discussed earlier) to classify each word on it's positivity/negativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[48 48]\n",
      " [18 86]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, random_state = 42)\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "confusionMatrix_NB = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusionMatrix_NB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try other classification models on our bag of words such as Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82 14]\n",
      " [42 62]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "confusionMatrix_RF = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusionMatrix_RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a Random Forest of CART trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[83 13]\n",
      " [47 57]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'gini', random_state = 42)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "confusionMatrix_CART = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(confusionMatrix_CART)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also evaluate our models based on Accuracy, Precision, Recall, and the F1 Score\n",
    "\n",
    "As a quick refresher:\n",
    "* Accuracy is the total number of correct predictions\n",
    "* Precision is a measure of how many predicted positives are actually positive (How useful are the results?)\n",
    "* Recall is a measure of how many positives were correctly identified (How complete are the results?)\n",
    "* F1 is the harmonic mean of Precision and Recall (usefull & complete) and ranges from 1 (best) to 0 (worst)\n",
    "\n",
    "More info can be found in the relevant Wikipedia articles:\n",
    "* [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)\n",
    "* [Precision & Recall](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "* [F1 Score](https://en.wikipedia.org/wiki/F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy: 67.0%\n",
      "Naive Bayes Precision: 82.7%\n",
      "Naive Bayes Recall: 64.2%\n",
      "Naive Bayes F1: 0.723\n"
     ]
    }
   ],
   "source": [
    "accuracy_NB = (confusionMatrix_NB[1,1]+confusionMatrix_NB[0,0])/len(y_pred)\n",
    "precision_NB = confusionMatrix_NB[1,1]/(confusionMatrix_NB[1,1] + confusionMatrix_NB[1,0])\n",
    "recall_NB = confusionMatrix_NB[1,1]/(confusionMatrix_NB[1,1] + confusionMatrix_NB[0,1])\n",
    "f1_NB = (2 * precision_NB * recall_NB) / (precision_NB + recall_NB)\n",
    "print(f'Naive Bayes Accuracy: {accuracy_NB*100}%')\n",
    "print(f'Naive Bayes Precision: {precision_NB*100:0.1f}%')\n",
    "print(f'Naive Bayes Recall: {recall_NB*100:0.1f}%')\n",
    "print(f'Naive Bayes F1: {f1_NB:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 72.0%\n",
      "Random Forest Precision: 59.6%\n",
      "Random Forest Recall: 81.6%\n",
      "Random Forest F1: 0.689\n"
     ]
    }
   ],
   "source": [
    "accuracy_RF = (confusionMatrix_RF[1,1]+confusionMatrix_RF[0,0])/len(y_pred)\n",
    "precision_RF = confusionMatrix_RF[1,1]/(confusionMatrix_RF[1,1] + confusionMatrix_RF[1,0])\n",
    "recall_RF = confusionMatrix_RF[1,1]/(confusionMatrix_RF[1,1] + confusionMatrix_RF[0,1])\n",
    "f1_RF = (2 * precision_RF * recall_RF) / (precision_RF + recall_RF)\n",
    "print(f'Random Forest Accuracy: {accuracy_RF*100}%')\n",
    "print(f'Random Forest Precision: {precision_RF*100:0.1f}%')\n",
    "print(f'Random Forest Recall: {recall_RF*100:0.1f}%')\n",
    "print(f'Random Forest F1: {f1_RF:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Accuracy: 70.0%\n",
      "CART Precision: 54.8%\n",
      "CART Recall: 81.4%\n",
      "CART F1: 0.655\n"
     ]
    }
   ],
   "source": [
    "accuracy_CART = (confusionMatrix_CART[1,1]+confusionMatrix_CART[0,0])/len(y_pred)\n",
    "precision_CART = confusionMatrix_CART[1,1]/(confusionMatrix_CART[1,1] + confusionMatrix_CART[1,0])\n",
    "recall_CART = confusionMatrix_CART[1,1]/(confusionMatrix_CART[1,1] + confusionMatrix_CART[0,1])\n",
    "f1_CART = (2 * precision_CART * recall_CART) / (precision_CART + recall_CART)\n",
    "print(f'CART Accuracy: {accuracy_CART*100}%')\n",
    "print(f'CART Precision: {precision_CART*100:0.1f}%')\n",
    "print(f'CART Recall: {recall_CART*100:0.1f}%')\n",
    "print(f'CART F1: {f1_CART:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can see that Naive Bayes seems to perform the best on this dataset and does relatively well given the limited size of the dataset and sparsity of the matrix.\n",
    "\n",
    "Ways to reduce the sparcity of the matrix by reducing the number of features, words in this case, will be covered in future articles.\n",
    "\n",
    "P.S. If you are having trouble getting the required NLTK packages, below is some code that can be used for downloading the NLTK packages. [Source](https://stackoverflow.com/a/50406704)\n",
    "```\n",
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
