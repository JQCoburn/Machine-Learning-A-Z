{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z: Part 1 - Data Preprocessing\n",
    "\n",
    "This section of the Machine Learning A-Z Course focuses on how to pre-process the data to prepare it for analysis\n",
    "\n",
    "## Step 1. Import the required libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Libraries for fast linear algebra and array manipulation\n",
    "import pandas as pd # Import and manage datasets\n",
    "from plotly import __version__ as py__version__\n",
    "import plotly.express as px # Libraries for ploting data\n",
    "from sklearn import __version__ as skl__version__\n",
    "from sklearn.impute import SimpleImputer # Library to impute replacement values for missing data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Libraries to do encoding of categorical variables\n",
    "from sklearn.compose import ColumnTransformer # Library to transform only certain columns/features at a time\n",
    "from sklearn.model_selection import train_test_split # Library to split data into training and test sets.\n",
    "from sklearn.preprocessing import StandardScaler # Library to do feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library versions used in this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.16.4\n",
      "Pandas: 0.25.1\n",
      "Plotly: 4.0.0\n",
      "Scikit-learn: 0.21.2\n"
     ]
    }
   ],
   "source": [
    "print('Numpy: ' + np.__version__)\n",
    "print('Pandas: ' + pd.__version__)\n",
    "print('Plotly: ' + py__version__)\n",
    "print('Scikit-learn: ' + skl__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 4 columns):\n",
      "Country      10 non-null object\n",
      "Age          9 non-null float64\n",
      "Salary       9 non-null float64\n",
      "Purchased    10 non-null object\n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 448.0+ bytes\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def LoadData():\n",
    "    dataset = pd.read_csv('Data.csv')\n",
    "    return dataset\n",
    "\n",
    "dataset = LoadData()\n",
    "print(dataset.head(3))\n",
    "print()\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset contains 4 columns:\n",
    "* Country - String\n",
    "* Age - Float\n",
    "* Salary - Float\n",
    "* Purchased - String\n",
    "This file appears to represent some data about a company's customers including their country, age, salary and whether or not they bought the product.\n",
    "\n",
    "Additionally we can see that we have 10 records, but there seems to some data missing in the age and salary columns. We'll deal with that in a minute.\n",
    "\n",
    "## Step 3. Split the data into features and outputs (independent variables and dependent variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,:-1].values # All the columns except the last are features\n",
    "y = dataset.iloc[:,-1].values # The last column is the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Handle Missing Data\n",
    "As we noted earlier, we are missing data in both the age and salary variables.\n",
    "\n",
    "One common way to hanlde missing data is to replace the missing data with the mean of the data. We can easily do this with tools from scikit learn\n",
    "\n",
    "*__Question:__ What do you do for a categorical variable?* Options on SimpleImputer include:\n",
    "* Mean - Numeric Only\n",
    "* Median - Numeric Only\n",
    "* Most_Frequent - Sort of like avg for categorical? Might work on large data sets\n",
    "* Constant - Fill will some value of your choosing\n",
    "\n",
    "*__Question:__ Doesn't filling the empty spots with mean values before splitting test and train datasets give the training set some information about the test set?*\n",
    "\n",
    "*__Answer:__ Yup! It's called Data Snoop Bias. In this case our dataset is so small we don't have much choice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.77777777778]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.77777777777778 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:,1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Encode Categorical Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 1 2 1 0 2 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "labelencoder_X = LabelEncoder()\n",
    "x = labelencoder_X.fit_transform(X[:,0])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code has converted our string labels into numbers, however these numbers imply an order or hierarchry to the categories which does not actually exist. (It does exist in the case of ordinal variables though such as size: small, medium, large). This is a problem because many machine learning algorithms will pick up on this implied order and perform poorly since it is an artifact of how we prepared the data, not a relationship that actually exists in the data. To fix this we will one-hot encode this categorical variable.\n",
    "\n",
    "### One-hot Encoding\n",
    "\n",
    "In one-hot encoding, the categorical variable is removed and a new variable (column) for each value of the original variable is added (in our example 3 columns will be added). Then each record simple gets a one in the column that matches the value of the original variable and a zero in the other new variables. These new columns now represent whether a particular record belongs to a certain group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 44.0 72000.0]\n",
      " [0.0 0.0 1.0 27.0 48000.0]\n",
      " [0.0 1.0 0.0 30.0 54000.0]\n",
      " [0.0 0.0 1.0 38.0 61000.0]\n",
      " [0.0 1.0 0.0 40.0 63777.77777777778]\n",
      " [1.0 0.0 0.0 35.0 58000.0]\n",
      " [0.0 0.0 1.0 38.77777777777778 52000.0]\n",
      " [1.0 0.0 0.0 48.0 79000.0]\n",
      " [0.0 1.0 0.0 50.0 83000.0]\n",
      " [1.0 0.0 0.0 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "columntransformer = ColumnTransformer(\n",
    "    [('Country_Category', OneHotEncoder(), [0])],\n",
    "    remainder = 'passthrough')\n",
    "\n",
    "X = np.array(columntransformer.fit_transform(X))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to one-hot encode the dependent variable, but since it already only has 2 values, we only need to use a *LabelEncoder*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 1 0 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "labelencoder_Y = LabelEncoder()\n",
    "y = labelencoder_Y.fit_transform(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Split the Data into Test and Training Sets\n",
    "\n",
    "The training set is used to teach the machine learning model the relationships and patterns present in the data. The test set is used to evaluate the trained model and see how well it can perform on data it has never seen before. \n",
    "\n",
    "If the model performs well on both the training and test sets we can expect is has done a good job of learning the patterns present in the data and will generalize to new data well. (This assumes that the original dataset is representative of the new dataset and we have avoided all significant sources of bias.) \n",
    "\n",
    "If the model performs well on training data, but not on the test data, then it is likely that the model has overfit our test data, or in other words it has memorized the dataset instead of learning the patterns with-in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7. Scale the features\n",
    "\n",
    "Many Machine Learning algorithms and models deal with the __Euclidean Distance__ (Square root of the sum of the squares of the difference) between observations. This means that to make it easier for our models to learn and perform well, each feature should have a similar impact on the __Euclidean Distance__. In order for the features to have similar impacts, they need to have similar scales. However in our data, the age feature is in a scale of 10's while salaray is in a scale of 10,000's. This means that impact of even a large change in age will get lost in the impact of even a small change in income. Essentially we are implying to the model that income is significantly more important than age which can almost be ignored, however that may not be true! In order to give age and income equal importance we need to change the scale the features to be on the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.         -0.57735027 -0.57735027 -0.7529426  -0.62603778]\n",
      " [ 1.         -0.57735027 -0.57735027  1.00845381  1.01304295]\n",
      " [ 1.         -0.57735027 -0.57735027  1.79129666  1.83258331]\n",
      " [-1.          1.73205081 -0.57735027 -1.73149616 -1.09434656]\n",
      " [ 1.         -0.57735027 -0.57735027 -0.36152118  0.42765698]\n",
      " [-1.          1.73205081 -0.57735027  0.22561096  0.05040824]\n",
      " [-1.         -0.57735027  1.73205081 -0.16581046 -0.27480619]\n",
      " [-1.         -0.57735027  1.73205081 -0.01359102 -1.32850095]]\n",
      "\n",
      "[[-1.          1.73205081 -0.57735027  2.18271808  2.30089209]\n",
      " [-1.         -0.57735027  1.73205081 -2.3186283  -1.79680973]]\n"
     ]
    }
   ],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "print(X_train)\n",
    "print()\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some Machine Learning libraries in python do not require you to scale the features as they do it automatically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
