{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z: Section 24 Apriori Association Rule Learning\n",
    "\n",
    "In the course so far we have looked at Regression, Classification, and Clustering. This notebook is about Association Rule Learning. Association Rule learning looks for common connections in the data.\n",
    "\n",
    "For instance:\n",
    "* Purchases containing milk also usually contain bread\n",
    "* People who watch Mulan often also watch Tangled\n",
    "* People who listen to Mozart often also listen to Bach\n",
    "\n",
    "__Important Note:__\n",
    "A rule which applies in one direction may not always apply in the reverse. So although purchases containing milk often contain bread, it may not be true the transactions containing bread often contain milk.\n",
    "\n",
    "Association Rule Learning is all about finding these _A_ often implies _B_ type of relationships with-in our data.\n",
    "\n",
    "Apriori ARL find _A_ implies _B_ rules by looking at 3 values.\n",
    "1. Support(_A_): The percent of all records (transactions, users, etc.) which contain _A_.\n",
    "1. Confidence (_A_->_B_): The percent of records which contain _A_ which also contain _B_.\n",
    "1. Lift(_A_->_B_): The Confidence (_A_->_B_) divided by the Support(_B_)\n",
    "\n",
    "The Support, Confidence, and Lift are used in Apriori ARL models as follows:\n",
    "1. Calculate the Support for all items (products, movies, artists, etc.) in our records and select only those above a predetermined Support level. We do this for three reasons. First, if there are not enough records containing A, we won't be able to accurately find what A implies. Second, we are generally interested in finding rules for our most common items so we can leverage their influence. A strong rule that rarely applies often isn't very useful. Third, in the following steps we will look at combinations of items and looking at every possible combination of items can be computationally prohibitive for large datasets with many different items\n",
    "1. For the subset of items selected in the previous step, find the Confidence that _A_ implies _B_ for every combination in the subset. Select only the rules having higher than a predetermined confidence level. Confidence can be thought of as a numerical representation of our confidence that a rule _may_ exist between _A_ and _B_.\n",
    "1. For the subset of rules selected in the previous step, rank the rules by lift from highest to lowest. In this case lift tells us how strong a potential rule is. If a large number of records which contain _A_ also contain _B_, but _B_ is very common in the dataset overall (_B_ has a large support) it may not be significant that _B_ appears commonly with _A_ (small Lift). If _B_ is generally uncommon in the data (_B_ has a small support) it may be significant that it appears commonly with _A_ (large Lift)\n",
    "\n",
    "For our example data, we will be looking at the contents of purchases and looking for items which are often purchased together.\n",
    "\n",
    "## Step 1 Import and Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Libraries for fast linear algebra and array manipulation\n",
    "import pandas as pd # Import and manage datasets\n",
    "from plotly import __version__ as py__version__\n",
    "import plotly.express as px # Libraries for ploting data\n",
    "import plotly.graph_objects as go # Libraries for ploting data\n",
    "from sklearn import __version__ as skl__version__\n",
    "from sklearn.model_selection import train_test_split # Library to split data into training and test sets.\n",
    "from sklearn.preprocessing import StandardScaler # Library to do feature scaling\n",
    "from sklearn.tree import DecisionTreeClassifier # Library to do Decision Tree classification\n",
    "from sklearn.metrics import confusion_matrix #Function for computing the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library versions used in this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.16.4\n",
      "Pandas: 0.25.1\n",
      "Plotly: 4.0.0\n",
      "Scikit-learn: 0.21.2\n"
     ]
    }
   ],
   "source": [
    "print('Numpy: ' + np.__version__)\n",
    "print('Pandas: ' + pd.__version__)\n",
    "print('Plotly: ' + py__version__)\n",
    "print('Scikit-learn: ' + skl__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        0          1        2               3             4   \\\n",
      "0   shrimp    almonds  avocado  vegetables mix  green grapes   \n",
      "1  burgers  meatballs     eggs             NaN           NaN   \n",
      "2  chutney        NaN      NaN             NaN           NaN   \n",
      "\n",
      "                 5     6               7             8             9   \\\n",
      "0  whole weat flour  yams  cottage cheese  energy drink  tomato juice   \n",
      "1               NaN   NaN             NaN           NaN           NaN   \n",
      "2               NaN   NaN             NaN           NaN           NaN   \n",
      "\n",
      "               10         11     12     13             14      15  \\\n",
      "0  low fat yogurt  green tea  honey  salad  mineral water  salmon   \n",
      "1             NaN        NaN    NaN    NaN            NaN     NaN   \n",
      "2             NaN        NaN    NaN    NaN            NaN     NaN   \n",
      "\n",
      "                  16               17       18         19  \n",
      "0  antioxydant juice  frozen smoothie  spinach  olive oil  \n",
      "1                NaN              NaN      NaN        NaN  \n",
      "2                NaN              NaN      NaN        NaN  \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7501 entries, 0 to 7500\n",
      "Data columns (total 20 columns):\n",
      "0     7501 non-null object\n",
      "1     5747 non-null object\n",
      "2     4389 non-null object\n",
      "3     3345 non-null object\n",
      "4     2529 non-null object\n",
      "5     1864 non-null object\n",
      "6     1369 non-null object\n",
      "7     981 non-null object\n",
      "8     654 non-null object\n",
      "9     395 non-null object\n",
      "10    256 non-null object\n",
      "11    154 non-null object\n",
      "12    87 non-null object\n",
      "13    47 non-null object\n",
      "14    25 non-null object\n",
      "15    8 non-null object\n",
      "16    4 non-null object\n",
      "17    4 non-null object\n",
      "18    3 non-null object\n",
      "19    1 non-null object\n",
      "dtypes: object(20)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def LoadData():\n",
    "    dataset = pd.read_csv('Market_Basket_Optimisation.csv', header = None)\n",
    "    return dataset\n",
    "\n",
    "dataset = LoadData()\n",
    "print(dataset.head(3))\n",
    "print()\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the raw data we can see that each row represents a shopping basket and has the items contained in that basket in the columns. However, the Apriori ARL library we are using expects a list of lists with each inner list representing a basket so we will need to transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "baskets = []\n",
    "for i in range(0,len(dataset)):\n",
    "    basket = []\n",
    "    for j in range(0,len(dataset.columns)):\n",
    "        if dataset.iloc[i,j] == dataset.iloc[i,j]: #This is kind of an odd check of np.NaN which is used for missing values does not equal itself and hence this will be false.\n",
    "            basket.append(dataset.iloc[i,j])\n",
    "    baskets.append(basket)\n",
    "    \n",
    "#print(baskets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apyori import apriori\n",
    "rules = apriori(baskets, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule: light cream -> chicken\n",
      "Support: 0.004532728969470737\n",
      "Confidence: 0.29059829059829057\n",
      "Lift: 4.84395061728395\n",
      "=====================================\n",
      "Rule: mushroom cream sauce -> escalope\n",
      "Support: 0.005732568990801226\n",
      "Confidence: 0.3006993006993007\n",
      "Lift: 3.790832696715049\n",
      "=====================================\n",
      "Rule: escalope -> pasta\n",
      "Support: 0.005865884548726837\n",
      "Confidence: 0.3728813559322034\n",
      "Lift: 4.700811850163794\n",
      "=====================================\n",
      "Rule: honey -> fromage blanc\n",
      "Support: 0.003332888948140248\n",
      "Confidence: 0.2450980392156863\n",
      "Lift: 5.164270764485569\n",
      "=====================================\n",
      "Rule: herb & pepper -> ground beef\n",
      "Support: 0.015997866951073192\n",
      "Confidence: 0.3234501347708895\n",
      "Lift: 3.2919938411349285\n",
      "=====================================\n"
     ]
    }
   ],
   "source": [
    "results = list(rules)\n",
    "for item in results[0:5]:\n",
    "\n",
    "    # first index of the inner list\n",
    "    # Contains base item and add item\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    print(\"Rule: \" + items[0] + \" -> \" + items[1])\n",
    "\n",
    "    #second index of the inner list\n",
    "    print(\"Support: \" + str(item[1]))\n",
    "\n",
    "    #third index of the list located at 0th\n",
    "    #of the third index of the inner list\n",
    "\n",
    "    print(\"Confidence: \" + str(item[2][0][2]))\n",
    "    print(\"Lift: \" + str(item[2][0][3]))\n",
    "    print(\"=====================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the first result, we can understand the following:\n",
    "* 0.45% of the all the baskets contained Light Cream which corresponds to 34 baskets out of the 7501 total\n",
    "* 29% of the baskets containing Light Cream also contained Chicken\n",
    "* Chicken occured in baskets containing light cream 4.8 times more frequently than it occured in the dataset overall\n",
    "\n",
    "From this we could try to put Light Cream and Chicken close together in our store try to improve chicken sales, or we coul try to distance Light Cream and Chicken so people looking for both would see more of the products we are selling and pick-up something they wouldn't have otherwise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
