{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z: Section 29 Artificial Neural Network\n",
    "\n",
    "In this notebook we'll be using an Artificial Neural Network to solve a business problem described later. First though, we'll cover a quick description of what a neural network is and how it works. Neural Networks though are an entire area of study and a branch of Machine Learning on their own so we'll only give a high level explanation of them here.\n",
    "\n",
    "Artificial networks consist of groups of little units called neurons (after the ones in your brain!). A single neuron takes inputs from other neurons, multiplies each input by a weight (more on that in a moment), sums the resulting weighted inputs and then calculates an activation (i.e. on or off) from that sum. An artificial neural network is composed of many layers of the neurons, each layer passing their outputs as input to the next layer. The last layer (called the output layer) is used to pass the results back to the user.\n",
    "\n",
    "In order for a neural network to be useful it needs to be trained. However, before training the developer needs to decide the structure of the network (how many layers, how many neurons in each layer, what to use as an activation function, etc.). After the structure is setup the weight for each linkage is initialized to a random value and the network is ready to be trained. \n",
    "\n",
    "To train the neural network, it is fed the training data and the output (which will initially be random) is compared to the expected output. The error in the prediction is computed and then using gradient-descent and back-propogration the weights are adjusted to make the output a little closer to the true output. Then the process is repeated on the next piece of training data, and the next, and the next and so-on. Once the network has been trained with all the available training data, that completes a single training epoch. The training algorithm then repeats epochs a set number of times always inching closer a better solution. Be careful though it is possible to overfit your training data by training on it too long. The length of time you can train on your dataset will vary by how large the dataset is, how large the network is, how many independent variables there are, how you subdivide the training data, etc. However, You'll be able to identify overfitting by evaluating the performance of the model on a test set after every epoch and looking for a drop in the accuracy of the predictions.\n",
    "\n",
    "The problem we'll be solving with the ANN will be one of trying to determine which users are likely to stop using a particular bank (churn). In this case we'll use an ANN to create a geodemographic model from a sample of data the bank collected about it's customers and try to identify who is likely to leave the bank.\n",
    "\n",
    "## Step 1 Import and Prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # Libraries for fast linear algebra and array manipulation\n",
    "import pandas as pd # Import and manage datasets\n",
    "from plotly import __version__ as py__version__\n",
    "import plotly.express as px # Libraries for ploting data\n",
    "import plotly.graph_objects as go # Libraries for ploting data\n",
    "from sklearn import __version__ as skl__version__\n",
    "from sklearn.model_selection import train_test_split # Library to split data into training and test sets.\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder # Libraries to do encoding of categorical variables\n",
    "from sklearn.compose import ColumnTransformer # Library to transform only certain columns/features at a time\n",
    "from sklearn.preprocessing import StandardScaler # Library to do feature scaling\n",
    "from sklearn.metrics import confusion_matrix #Function for computing the confusion matrix\n",
    "from tensorflow import __version__ as tf__version__\n",
    "from tensorflow import keras # High level library for building Neural Networks\n",
    "from tensorflow.keras.models import Sequential # Keras module for building a neural network with sequential layers\n",
    "from tensorflow.keras.layers import Dense #Keras module for building a neural network with fully interconnected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library versions used in this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy: 1.16.4\n",
      "Pandas: 0.25.1\n",
      "Plotly: 4.0.0\n",
      "Scikit-learn: 0.21.2\n",
      "Tensorflow Verion: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "print('Numpy: ' + np.__version__)\n",
    "print('Pandas: ' + pd.__version__)\n",
    "print('Plotly: ' + py__version__)\n",
    "print('Scikit-learn: ' + skl__version__)\n",
    "print('Tensorflow Verion: ' + tf__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0       2       0.00              1          1               1   \n",
      "1       1   83807.86              1          0               1   \n",
      "2       8  159660.80              3          1               0   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      "RowNumber          10000 non-null int64\n",
      "CustomerId         10000 non-null int64\n",
      "Surname            10000 non-null object\n",
      "CreditScore        10000 non-null int64\n",
      "Geography          10000 non-null object\n",
      "Gender             10000 non-null object\n",
      "Age                10000 non-null int64\n",
      "Tenure             10000 non-null int64\n",
      "Balance            10000 non-null float64\n",
      "NumOfProducts      10000 non-null int64\n",
      "HasCrCard          10000 non-null int64\n",
      "IsActiveMember     10000 non-null int64\n",
      "EstimatedSalary    10000 non-null float64\n",
      "Exited             10000 non-null int64\n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def LoadData():\n",
    "    dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "    return dataset\n",
    "\n",
    "dataset = LoadData()\n",
    "print(dataset.head(3))\n",
    "print()\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dataset contains 14 columns. However, not all of them are useful for out model. Only the columns listed below will actually be useful:\n",
    "* CreditScore\n",
    "* Geography\n",
    "* Gender\n",
    "* Age\n",
    "* Tenure (How long the person has been a customer)\n",
    "* Balance\n",
    "* NumOfProducts (How many of the bank's products does the customer use)\n",
    "* HasCrCard\n",
    "* IsActiveMember\n",
    "* Estimated Salary\n",
    "\n",
    "Using the data in these columns we'll try to predict the value in the *Exited* column to determine if a user will leave the bank soon or not.\n",
    "\n",
    "You'll notice that some of these columns are categorical variables that we'll need to encode to work properly. Also there does not appear to be any missing data in this data set.\n",
    "\n",
    "## Step 2. Split and Encode the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:,3:-1].values # All the columns except the last are features\n",
    "y = dataset.iloc[:,-1].values # The last column is the dependent variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've split the data into dependent and independent datasets we need to encode the categorical variables in the independent variables.\n",
    "\n",
    "We'll use One-Hot encoding on both gender and country to encode the categorical data. Don't forget to remove one of the new columns from the one-hot encoded categorical variables to avoid the dummy variable trap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 1.0 0.0 ... 0 1 112542.58]\n",
      " [0.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [0.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [1.0 0.0 1.0 ... 1 0 92888.52]\n",
      " [0.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "columntransformer = ColumnTransformer([\n",
    "    ('Country_Category', OneHotEncoder(drop='first'), [1]),\n",
    "    ('Gender_Category', OneHotEncoder(drop='first'), [2])],\n",
    "    remainder = 'passthrough')\n",
    "X = np.array(columntransformer.fit_transform(X))\n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to split the data into test and training sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to scale the data to easy the computations we'll do on the data and prevent bias from columns which tend to have larger numbers & variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57946723 -0.57638802  0.91324755 ...  0.64920267  0.97481699\n",
      "   1.36766974]\n",
      " [ 1.72572313 -0.57638802  0.91324755 ...  0.64920267  0.97481699\n",
      "   1.6612541 ]\n",
      " [-0.57946723  1.73494238  0.91324755 ...  0.64920267 -1.02583358\n",
      "  -0.25280688]\n",
      " ...\n",
      " [-0.57946723 -0.57638802 -1.09499335 ... -1.54035103 -1.02583358\n",
      "  -0.1427649 ]\n",
      " [-0.57946723 -0.57638802  0.91324755 ...  0.64920267 -1.02583358\n",
      "  -0.05082558]\n",
      " [ 1.72572313 -0.57638802  0.91324755 ...  0.64920267  0.97481699\n",
      "  -0.81456811]]\n",
      "\n",
      "[[ 1.72572313 -0.57638802  0.91324755 ... -1.54035103 -1.02583358\n",
      "  -1.01960511]\n",
      " [-0.57946723 -0.57638802  0.91324755 ...  0.64920267  0.97481699\n",
      "   0.79888291]\n",
      " [-0.57946723  1.73494238 -1.09499335 ...  0.64920267 -1.02583358\n",
      "  -0.72797953]\n",
      " ...\n",
      " [-0.57946723 -0.57638802 -1.09499335 ...  0.64920267 -1.02583358\n",
      "  -1.16591585]\n",
      " [-0.57946723 -0.57638802  0.91324755 ...  0.64920267 -1.02583358\n",
      "  -0.41163463]\n",
      " [ 1.72572313 -0.57638802  0.91324755 ...  0.64920267  0.97481699\n",
      "   0.12593183]]\n"
     ]
    }
   ],
   "source": [
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.transform(X_test)\n",
    "\n",
    "print(X_train)\n",
    "print()\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our dataset is ready for use and we can put together the structure of our ANN!\n",
    "## Step 3. Design the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu', input_shape=(11,))) # First Hidden layer. We'll just default to the average of the input size (11) and output size (1)\n",
    "classifier.add(Dense(6, kernel_initializer = 'uniform', activation = 'relu')) # Second Hidden Layer\n",
    "classifier.add(Dense(1, kernel_initializer = 'uniform', activation = 'sigmoid')) # Final output Layer. If your output has multiple categories (instead of binary), you'll need use the softmax activation function and have an output neuron for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 3s 326us/sample - loss: 0.4941 - accuracy: 0.7941\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 2s 278us/sample - loss: 0.4292 - accuracy: 0.7945\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 2s 285us/sample - loss: 0.4248 - accuracy: 0.7945\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 2s 289us/sample - loss: 0.4162 - accuracy: 0.8120\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 2s 278us/sample - loss: 0.4070 - accuracy: 0.8281\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 2s 273us/sample - loss: 0.3979 - accuracy: 0.8301\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 2s 288us/sample - loss: 0.3905 - accuracy: 0.8309\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3841 - accuracy: 0.8316\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 2s 289us/sample - loss: 0.3791 - accuracy: 0.8411\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 2s 291us/sample - loss: 0.3755 - accuracy: 0.8439\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 2s 288us/sample - loss: 0.3715 - accuracy: 0.8462\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 2s 278us/sample - loss: 0.3691 - accuracy: 0.8503\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 2s 285us/sample - loss: 0.3668 - accuracy: 0.8487\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3651 - accuracy: 0.8505\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 2s 285us/sample - loss: 0.3633 - accuracy: 0.8516\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 2s 284us/sample - loss: 0.3614 - accuracy: 0.8525\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 2s 282us/sample - loss: 0.3596 - accuracy: 0.8530\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 2s 278us/sample - loss: 0.3587 - accuracy: 0.8558\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 2s 282us/sample - loss: 0.3576 - accuracy: 0.8544\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 2s 279us/sample - loss: 0.3565 - accuracy: 0.8541\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 2s 292us/sample - loss: 0.3560 - accuracy: 0.8551\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 2s 281us/sample - loss: 0.3551 - accuracy: 0.8553\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 2s 286us/sample - loss: 0.3547 - accuracy: 0.8540\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 2s 289us/sample - loss: 0.3540 - accuracy: 0.8570\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 2s 274us/sample - loss: 0.3531 - accuracy: 0.8585\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 2s 280us/sample - loss: 0.3531 - accuracy: 0.8554\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3519 - accuracy: 0.8594\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 2s 278us/sample - loss: 0.3513 - accuracy: 0.8572\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 2s 285us/sample - loss: 0.3507 - accuracy: 0.8581\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 2s 287us/sample - loss: 0.3508 - accuracy: 0.8572\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 2s 284us/sample - loss: 0.3501 - accuracy: 0.8595\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3497 - accuracy: 0.8577\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 2s 281us/sample - loss: 0.3495 - accuracy: 0.8562\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 2s 286us/sample - loss: 0.3488 - accuracy: 0.8605\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 2s 286us/sample - loss: 0.3491 - accuracy: 0.8594\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 2s 282us/sample - loss: 0.3493 - accuracy: 0.8576\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3479 - accuracy: 0.8577\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 2s 307us/sample - loss: 0.3481 - accuracy: 0.8608\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 2s 295us/sample - loss: 0.3492 - accuracy: 0.8605\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 2s 292us/sample - loss: 0.3481 - accuracy: 0.8601\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 2s 281us/sample - loss: 0.3482 - accuracy: 0.8611\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 2s 301us/sample - loss: 0.3471 - accuracy: 0.8602\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 2s 294us/sample - loss: 0.3476 - accuracy: 0.8608\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 2s 296us/sample - loss: 0.3470 - accuracy: 0.8610\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 2s 289us/sample - loss: 0.3462 - accuracy: 0.8614\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3463 - accuracy: 0.8597\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 2s 273us/sample - loss: 0.3463 - accuracy: 0.8585\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3462 - accuracy: 0.8586\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 2s 274us/sample - loss: 0.3462 - accuracy: 0.8602\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3463 - accuracy: 0.8605\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 2s 295us/sample - loss: 0.3458 - accuracy: 0.8595\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 2s 293us/sample - loss: 0.3470 - accuracy: 0.8590\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 2s 269us/sample - loss: 0.3465 - accuracy: 0.8584\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 2s 277us/sample - loss: 0.3465 - accuracy: 0.8581\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 2s 294us/sample - loss: 0.3458 - accuracy: 0.8594\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 2s 299us/sample - loss: 0.3459 - accuracy: 0.8576\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 2s 288us/sample - loss: 0.3458 - accuracy: 0.8612\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 2s 303us/sample - loss: 0.3457 - accuracy: 0.8596\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 2s 301us/sample - loss: 0.3452 - accuracy: 0.8597\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 2s 294us/sample - loss: 0.3456 - accuracy: 0.8577\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 2s 291us/sample - loss: 0.3446 - accuracy: 0.8618\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 2s 294us/sample - loss: 0.3459 - accuracy: 0.8596\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 2s 291us/sample - loss: 0.3452 - accuracy: 0.8594\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 2s 299us/sample - loss: 0.3441 - accuracy: 0.8600\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 2s 288us/sample - loss: 0.3457 - accuracy: 0.8596\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 2s 296us/sample - loss: 0.3448 - accuracy: 0.8612\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 2s 301us/sample - loss: 0.3445 - accuracy: 0.8621\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3447 - accuracy: 0.8614\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 2s 293us/sample - loss: 0.3447 - accuracy: 0.8602\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 2s 297us/sample - loss: 0.3449 - accuracy: 0.8597\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 2s 305us/sample - loss: 0.3446 - accuracy: 0.8601\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 2s 292us/sample - loss: 0.3445 - accuracy: 0.8608\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 2s 288us/sample - loss: 0.3450 - accuracy: 0.8609\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 2s 279us/sample - loss: 0.3446 - accuracy: 0.8602\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 2s 269us/sample - loss: 0.3453 - accuracy: 0.8625\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 2s 275us/sample - loss: 0.3436 - accuracy: 0.8606\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 2s 302us/sample - loss: 0.3442 - accuracy: 0.8606\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 2s 292us/sample - loss: 0.3433 - accuracy: 0.8602\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 2s 272us/sample - loss: 0.3447 - accuracy: 0.8618\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3440 - accuracy: 0.8601\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 2s 280us/sample - loss: 0.3437 - accuracy: 0.8614\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 2s 280us/sample - loss: 0.3438 - accuracy: 0.8585\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 2s 280us/sample - loss: 0.3438 - accuracy: 0.8602\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 2s 286us/sample - loss: 0.3430 - accuracy: 0.8633\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 2s 301us/sample - loss: 0.3435 - accuracy: 0.8616\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 2s 296us/sample - loss: 0.3438 - accuracy: 0.8618\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 2s 283us/sample - loss: 0.3440 - accuracy: 0.8601\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 2s 293us/sample - loss: 0.3438 - accuracy: 0.8611\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 2s 289us/sample - loss: 0.3435 - accuracy: 0.8612\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 2s 288us/sample - loss: 0.3433 - accuracy: 0.8615\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3441 - accuracy: 0.8614\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 2s 302us/sample - loss: 0.3435 - accuracy: 0.8587\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 2s 296us/sample - loss: 0.3442 - accuracy: 0.8606\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 2s 297us/sample - loss: 0.3434 - accuracy: 0.8605\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 2s 287us/sample - loss: 0.3443 - accuracy: 0.8609\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 2s 293us/sample - loss: 0.3440 - accuracy: 0.8606\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 2s 292us/sample - loss: 0.3442 - accuracy: 0.8621\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 2s 289us/sample - loss: 0.3434 - accuracy: 0.8630\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 2s 290us/sample - loss: 0.3435 - accuracy: 0.8605\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 2s 293us/sample - loss: 0.3438 - accuracy: 0.8610\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0e981b2d90>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=X_train, y=y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Evaluate the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False]\n",
      " [False]\n",
      " [False]\n",
      " ...\n",
      " [ True]\n",
      " [False]\n",
      " [False]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = (classifier.predict(X_test) > 0.5)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1533   74]\n",
      " [ 216  177]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can finally see that the accuracy of our neural network on the training set ends up around 86%. Looking at the confusion matrix we see that the we are correctly able to identify 85.5% of the users who will leave the bank. This shows that our model fits the data well and is not overfit.\n",
    "\n",
    "The bank could now use this ANN to identify their customers who are most likely to leave the bank and dig into those users to look for patterns that may identify why and what the bank could do to retain those users most likely to leave. Very valuable insights indeed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
