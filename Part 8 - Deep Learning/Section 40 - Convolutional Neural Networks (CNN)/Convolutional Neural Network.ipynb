{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning A-Z: Section 30 Convolutional Neural Networks\n",
    "\n",
    "In this notebook we'll be using a Convolutional Neural Network to classify pictures between cats and dogs.\n",
    "\n",
    "Convolutional Neural Networks are a variation of Artificial Neural Networks which add a few new layers types to the beginning a standard neural network. CNNs are typically used for image processing and are specialised to do so. The new layers start with a convolution layer which looks at small groups of pixels to highlight features (such as vertical or horizontal lines). It then passes through a Rectified Linear activation (ReLu) layer which sharpens the contrast on features found in the convolution layer. After the ReLu layer is the Max Pooling layer. This layer shrinks the sharpened convolutions into small images by again looking at small groups on pixels and keeping only the maximum value pixel. Finally the pixels are flattened (from a matrix into an array) and passed to a traditional neural network to finally classify the image.\n",
    "\n",
    "## Step 1 Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras # High level library for building Neural Networks\n",
    "from tensorflow.keras.models import Sequential # Keras module for building a neural network with sequential layers\n",
    "from tensorflow.keras.layers import Convolution2D # Keras module for building a Convolution layer\n",
    "from tensorflow.keras.layers import MaxPooling2D # Keras module for building a Max Pooling Layer\n",
    "from tensorflow.keras.layers import Flatten # Keras module for building a flattening layer\n",
    "from tensorflow.keras.layers import Dense #Keras module for building a neural network with fully interconnected layers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Keras module for doing image preprocessing\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library versions used in this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Verion: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow Verion: ' + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 Build CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_height = 256\n",
    "image_width = image_height\n",
    "\n",
    "classifier = Sequential()\n",
    "#Convolution Layer\n",
    "classifier.add(Convolution2D(32, (3,3), input_shape=(image_height,image_width,3), activation = 'relu'))\n",
    "#Pooling Layer\n",
    "classifier.add(MaxPooling2D())\n",
    "#Adding a second Convolution Layer\n",
    "classifier.add(Convolution2D(32, (3,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D())\n",
    "#Adding a third Convolution Layer\n",
    "classifier.add(Convolution2D(32, (3,3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D())\n",
    "#Flattening\n",
    "classifier.add(Flatten())\n",
    "#Classic ANN\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(128, activation = 'relu'))\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "#Compile the model\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Prepare the Images & Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n",
      "Found 2001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "        'dataset/training_set',\n",
    "        target_size=(image_height,image_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "        'dataset/test_set',\n",
    "        target_size=(image_height,image_width),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "  1/256 [..............................] - ETA: 13:01 - loss: 0.7057 - accuracy: 0.4062WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.116490). Check your callbacks.\n",
      "256/256 [==============================] - 111s 435ms/step - loss: 0.6514 - accuracy: 0.6108 - val_loss: 0.5818 - val_accuracy: 0.6990\n",
      "Epoch 2/50\n",
      "256/256 [==============================] - 104s 405ms/step - loss: 0.5722 - accuracy: 0.7001 - val_loss: 0.5217 - val_accuracy: 0.7329\n",
      "Epoch 3/50\n",
      "256/256 [==============================] - 110s 430ms/step - loss: 0.5160 - accuracy: 0.7439 - val_loss: 0.5388 - val_accuracy: 0.7162\n",
      "Epoch 4/50\n",
      "256/256 [==============================] - 105s 409ms/step - loss: 0.4651 - accuracy: 0.7765 - val_loss: 0.4971 - val_accuracy: 0.7703\n",
      "Epoch 5/50\n",
      "256/256 [==============================] - 105s 409ms/step - loss: 0.4530 - accuracy: 0.7889 - val_loss: 0.4719 - val_accuracy: 0.7659\n",
      "Epoch 6/50\n",
      "256/256 [==============================] - 101s 395ms/step - loss: 0.4270 - accuracy: 0.8007 - val_loss: 0.4497 - val_accuracy: 0.7919\n",
      "Epoch 7/50\n",
      "256/256 [==============================] - 102s 398ms/step - loss: 0.4000 - accuracy: 0.8115 - val_loss: 0.4433 - val_accuracy: 0.7841\n",
      "Epoch 8/50\n",
      "256/256 [==============================] - 102s 397ms/step - loss: 0.3857 - accuracy: 0.8257 - val_loss: 0.4251 - val_accuracy: 0.8116\n",
      "Epoch 9/50\n",
      "256/256 [==============================] - 102s 397ms/step - loss: 0.3823 - accuracy: 0.8289 - val_loss: 0.4302 - val_accuracy: 0.7993\n",
      "Epoch 10/50\n",
      "256/256 [==============================] - 103s 402ms/step - loss: 0.3518 - accuracy: 0.8409 - val_loss: 0.4662 - val_accuracy: 0.8082\n",
      "Epoch 11/50\n",
      "256/256 [==============================] - 117s 456ms/step - loss: 0.3465 - accuracy: 0.8458 - val_loss: 0.4750 - val_accuracy: 0.8003\n",
      "Epoch 12/50\n",
      "256/256 [==============================] - 103s 402ms/step - loss: 0.3240 - accuracy: 0.8595 - val_loss: 0.4535 - val_accuracy: 0.8141\n",
      "Epoch 13/50\n",
      "256/256 [==============================] - 114s 447ms/step - loss: 0.3126 - accuracy: 0.8643 - val_loss: 0.4723 - val_accuracy: 0.8008\n",
      "Epoch 14/50\n",
      "256/256 [==============================] - 118s 461ms/step - loss: 0.3071 - accuracy: 0.8644 - val_loss: 0.4591 - val_accuracy: 0.8106\n",
      "Epoch 15/50\n",
      "256/256 [==============================] - 102s 397ms/step - loss: 0.2870 - accuracy: 0.8718 - val_loss: 0.4584 - val_accuracy: 0.8146\n",
      "Epoch 16/50\n",
      "256/256 [==============================] - 105s 412ms/step - loss: 0.2778 - accuracy: 0.8811 - val_loss: 0.4584 - val_accuracy: 0.8013\n",
      "Epoch 17/50\n",
      "256/256 [==============================] - 114s 444ms/step - loss: 0.2577 - accuracy: 0.8906 - val_loss: 0.4851 - val_accuracy: 0.8155\n",
      "Epoch 18/50\n",
      "256/256 [==============================] - 102s 398ms/step - loss: 0.2381 - accuracy: 0.8999 - val_loss: 0.4638 - val_accuracy: 0.8205\n",
      "Epoch 19/50\n",
      "256/256 [==============================] - 99s 386ms/step - loss: 0.2346 - accuracy: 0.9030 - val_loss: 0.4974 - val_accuracy: 0.8249\n",
      "Epoch 20/50\n",
      "256/256 [==============================] - 108s 421ms/step - loss: 0.2124 - accuracy: 0.9130 - val_loss: 0.4890 - val_accuracy: 0.8160\n",
      "Epoch 21/50\n",
      "256/256 [==============================] - 119s 466ms/step - loss: 0.1999 - accuracy: 0.9198 - val_loss: 0.5378 - val_accuracy: 0.8254\n",
      "Epoch 22/50\n",
      "256/256 [==============================] - 105s 409ms/step - loss: 0.1994 - accuracy: 0.9183 - val_loss: 0.5335 - val_accuracy: 0.8214\n",
      "Epoch 23/50\n",
      "256/256 [==============================] - 100s 391ms/step - loss: 0.1820 - accuracy: 0.9250 - val_loss: 0.5451 - val_accuracy: 0.8023\n",
      "Epoch 24/50\n",
      "256/256 [==============================] - 99s 385ms/step - loss: 0.1717 - accuracy: 0.9337 - val_loss: 0.5714 - val_accuracy: 0.8082\n",
      "Epoch 25/50\n",
      "256/256 [==============================] - 100s 389ms/step - loss: 0.1713 - accuracy: 0.9329 - val_loss: 0.5935 - val_accuracy: 0.8170\n",
      "Epoch 26/50\n",
      "256/256 [==============================] - 99s 388ms/step - loss: 0.1551 - accuracy: 0.9371 - val_loss: 0.6672 - val_accuracy: 0.8126\n",
      "Epoch 27/50\n",
      "256/256 [==============================] - 99s 387ms/step - loss: 0.1624 - accuracy: 0.9358 - val_loss: 0.6263 - val_accuracy: 0.8111\n",
      "Epoch 28/50\n",
      "256/256 [==============================] - 98s 382ms/step - loss: 0.1555 - accuracy: 0.9397 - val_loss: 0.5961 - val_accuracy: 0.8106\n",
      "Epoch 29/50\n",
      "256/256 [==============================] - 98s 381ms/step - loss: 0.1284 - accuracy: 0.9526 - val_loss: 0.6890 - val_accuracy: 0.8175\n",
      "Epoch 30/50\n",
      "256/256 [==============================] - 99s 385ms/step - loss: 0.1365 - accuracy: 0.9447 - val_loss: 0.6548 - val_accuracy: 0.8165\n",
      "Epoch 31/50\n",
      "256/256 [==============================] - 99s 387ms/step - loss: 0.1157 - accuracy: 0.9567 - val_loss: 0.6978 - val_accuracy: 0.8101\n",
      "Epoch 32/50\n",
      "256/256 [==============================] - 98s 383ms/step - loss: 0.1197 - accuracy: 0.9574 - val_loss: 0.6090 - val_accuracy: 0.8111\n",
      "Epoch 33/50\n",
      "256/256 [==============================] - 100s 390ms/step - loss: 0.1167 - accuracy: 0.9563 - val_loss: 0.6418 - val_accuracy: 0.8214\n",
      "Epoch 34/50\n",
      "256/256 [==============================] - 116s 455ms/step - loss: 0.1108 - accuracy: 0.9578 - val_loss: 0.6501 - val_accuracy: 0.8082\n",
      "Epoch 35/50\n",
      "256/256 [==============================] - 123s 479ms/step - loss: 0.1146 - accuracy: 0.9565 - val_loss: 0.6545 - val_accuracy: 0.8214\n",
      "Epoch 36/50\n",
      "256/256 [==============================] - 123s 479ms/step - loss: 0.1103 - accuracy: 0.9606 - val_loss: 0.6833 - val_accuracy: 0.8155\n",
      "Epoch 37/50\n",
      "256/256 [==============================] - 110s 431ms/step - loss: 0.0924 - accuracy: 0.9663 - val_loss: 0.7405 - val_accuracy: 0.8180\n",
      "Epoch 38/50\n",
      "256/256 [==============================] - 121s 473ms/step - loss: 0.1008 - accuracy: 0.9651 - val_loss: 0.6912 - val_accuracy: 0.8146\n",
      "Epoch 39/50\n",
      "256/256 [==============================] - 107s 418ms/step - loss: 0.0898 - accuracy: 0.9670 - val_loss: 0.7293 - val_accuracy: 0.8214\n",
      "Epoch 40/50\n",
      "256/256 [==============================] - 99s 388ms/step - loss: 0.0960 - accuracy: 0.9628 - val_loss: 0.7025 - val_accuracy: 0.8224\n",
      "Epoch 41/50\n",
      "256/256 [==============================] - 99s 387ms/step - loss: 0.0895 - accuracy: 0.9683 - val_loss: 0.8325 - val_accuracy: 0.7954\n",
      "Epoch 42/50\n",
      "256/256 [==============================] - 104s 408ms/step - loss: 0.0946 - accuracy: 0.9689 - val_loss: 0.6462 - val_accuracy: 0.8185\n",
      "Epoch 43/50\n",
      "256/256 [==============================] - 121s 471ms/step - loss: 0.0827 - accuracy: 0.9697 - val_loss: 0.7806 - val_accuracy: 0.8234\n",
      "Epoch 44/50\n",
      "256/256 [==============================] - 119s 466ms/step - loss: 0.0844 - accuracy: 0.9680 - val_loss: 0.6961 - val_accuracy: 0.8239\n",
      "Epoch 45/50\n",
      "256/256 [==============================] - 116s 452ms/step - loss: 0.0770 - accuracy: 0.9720 - val_loss: 0.7088 - val_accuracy: 0.8210\n",
      "Epoch 46/50\n",
      "256/256 [==============================] - 102s 399ms/step - loss: 0.0676 - accuracy: 0.9746 - val_loss: 0.8917 - val_accuracy: 0.8165\n",
      "Epoch 47/50\n",
      "256/256 [==============================] - 100s 389ms/step - loss: 0.0742 - accuracy: 0.9733 - val_loss: 0.7827 - val_accuracy: 0.8200\n",
      "Epoch 48/50\n",
      "256/256 [==============================] - 102s 398ms/step - loss: 0.0777 - accuracy: 0.9700 - val_loss: 0.7990 - val_accuracy: 0.8244\n",
      "Epoch 49/50\n",
      "256/256 [==============================] - 102s 397ms/step - loss: 0.0709 - accuracy: 0.9753 - val_loss: 0.7325 - val_accuracy: 0.8254\n",
      "Epoch 50/50\n",
      "256/256 [==============================] - 102s 398ms/step - loss: 0.0646 - accuracy: 0.9767 - val_loss: 0.7840 - val_accuracy: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f99d82c9410>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_dir=\"logs/fit_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "classifier.fit_generator(\n",
    "        training_set,\n",
    "        steps_per_epoch=256,\n",
    "        epochs=50,\n",
    "        callbacks=[tensorboard_callback],\n",
    "        validation_data=test_set,\n",
    "        validation_steps=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save('3Conv3Dense50Epochs.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We can see that by the end of 50 epochs, the CNN is able to correctly identify 97.7% of the images in the training set. However it can only identify 82.1% of the images in the test set. \n",
    "\n",
    "Looking at the logs on Tensorboard we can also see that the accuracy on the test set really didn't improve after epoch 20 and by epoch 10 it was performing significantly better on the training set than the test set.\n",
    "\n",
    "This is all very indicative of an over fit model. We should either reduce training time, play with the hyper parameters of the model, adjust the architecture of the model, or gather more data.\n",
    "\n",
    "We'll explore how to better tune Neural Networks in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
